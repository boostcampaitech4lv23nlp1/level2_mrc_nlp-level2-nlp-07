{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from lightning_model import LightningPCModel\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import argparse\n",
    "import transformers\n",
    "import datetime\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "\n",
    "import wandb\n",
    "import warnings\n",
    "import torch\n",
    "from os.path import join as pjoin\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.getcwd()\n",
    "MODEL_DIR = pjoin(ROOT_DIR, 'model_ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m parser \u001b[39m=\u001b[39m LightningPCModel\u001b[39m.\u001b[39madd_model_specific_args(parser)\n\u001b[1;32m     32\u001b[0m parser \u001b[39m=\u001b[39m Trainer\u001b[39m.\u001b[39madd_argparse_args(parser)\n\u001b[0;32m---> 33\u001b[0m args \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39;49mparse_args()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/argparse.py:1768\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1767\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_args\u001b[39m(\u001b[39mself\u001b[39m, args\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, namespace\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1768\u001b[0m     args, argv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_known_args(args, namespace)\n\u001b[1;32m   1769\u001b[0m     \u001b[39mif\u001b[39;00m argv:\n\u001b[1;32m   1770\u001b[0m         msg \u001b[39m=\u001b[39m _(\u001b[39m'\u001b[39m\u001b[39munrecognized arguments: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/argparse.py:1800\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1798\u001b[0m \u001b[39m# parse the arguments and exit if there are any errors\u001b[39;00m\n\u001b[1;32m   1799\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1800\u001b[0m     namespace, args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_known_args(args, namespace)\n\u001b[1;32m   1801\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(namespace, _UNRECOGNIZED_ARGS_ATTR):\n\u001b[1;32m   1802\u001b[0m         args\u001b[39m.\u001b[39mextend(\u001b[39mgetattr\u001b[39m(namespace, _UNRECOGNIZED_ARGS_ATTR))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/argparse.py:1841\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args\u001b[0;34m(self, arg_strings, namespace)\u001b[0m\n\u001b[1;32m   1836\u001b[0m         arg_string_pattern_parts\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39mA\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1838\u001b[0m \u001b[39m# otherwise, add the arg to the arg strings\u001b[39;00m\n\u001b[1;32m   1839\u001b[0m \u001b[39m# and note the index if it was an option\u001b[39;00m\n\u001b[1;32m   1840\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1841\u001b[0m     option_tuple \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_optional(arg_string)\n\u001b[1;32m   1842\u001b[0m     \u001b[39mif\u001b[39;00m option_tuple \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1843\u001b[0m         pattern \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mA\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/argparse.py:2157\u001b[0m, in \u001b[0;36mArgumentParser._parse_optional\u001b[0;34m(self, arg_string)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     args \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39moption\u001b[39m\u001b[39m'\u001b[39m: arg_string, \u001b[39m'\u001b[39m\u001b[39mmatches\u001b[39m\u001b[39m'\u001b[39m: options}\n\u001b[1;32m   2156\u001b[0m     msg \u001b[39m=\u001b[39m _(\u001b[39m'\u001b[39m\u001b[39mambiguous option: \u001b[39m\u001b[39m%(option)s\u001b[39;00m\u001b[39m could match \u001b[39m\u001b[39m%(matches)s\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 2157\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror(msg \u001b[39m%\u001b[39;49m args)\n\u001b[1;32m   2159\u001b[0m \u001b[39m# if exactly one action matched, this segmentation is good,\u001b[39;00m\n\u001b[1;32m   2160\u001b[0m \u001b[39m# so return the parsed action\u001b[39;00m\n\u001b[1;32m   2161\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(option_tuples) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/argparse.py:2519\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2510\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merror\u001b[39m(\u001b[39mself\u001b[39m, message):\n\u001b[1;32m   2511\u001b[0m     \u001b[39m\"\"\"error(message: string)\u001b[39;00m\n\u001b[1;32m   2512\u001b[0m \n\u001b[1;32m   2513\u001b[0m \u001b[39m    Prints a usage message incorporating the message to stderr and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2517\u001b[0m \u001b[39m    should either exit or raise an exception.\u001b[39;00m\n\u001b[1;32m   2518\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2519\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprint_usage(_sys\u001b[39m.\u001b[39;49mstderr)\n\u001b[1;32m   2520\u001b[0m     args \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mprog\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprog, \u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m: message}\n\u001b[1;32m   2521\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexit(\u001b[39m2\u001b[39m, _(\u001b[39m'\u001b[39m\u001b[39m%(prog)s\u001b[39;00m\u001b[39m: error: \u001b[39m\u001b[39m%(message)s\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39m%\u001b[39m args)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/argparse.py:2489\u001b[0m, in \u001b[0;36mArgumentParser.print_usage\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m   2487\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2488\u001b[0m     file \u001b[39m=\u001b[39m _sys\u001b[39m.\u001b[39mstdout\n\u001b[0;32m-> 2489\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_print_message(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_usage(), file)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/argparse.py:2455\u001b[0m, in \u001b[0;36mArgumentParser.format_usage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2452\u001b[0m formatter \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_formatter()\n\u001b[1;32m   2453\u001b[0m formatter\u001b[39m.\u001b[39madd_usage(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39musage, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_actions,\n\u001b[1;32m   2454\u001b[0m                     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mutually_exclusive_groups)\n\u001b[0;32m-> 2455\u001b[0m \u001b[39mreturn\u001b[39;00m formatter\u001b[39m.\u001b[39;49mformat_help()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/argparse.py:282\u001b[0m, in \u001b[0;36mHelpFormatter.format_help\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mformat_help\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 282\u001b[0m     help \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_root_section\u001b[39m.\u001b[39;49mformat_help()\n\u001b[1;32m    283\u001b[0m     \u001b[39mif\u001b[39;00m help:\n\u001b[1;32m    284\u001b[0m         help \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_long_break_matcher\u001b[39m.\u001b[39msub(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m, help)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/argparse.py:213\u001b[0m, in \u001b[0;36mHelpFormatter._Section.format_help\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformatter\u001b[39m.\u001b[39m_indent()\n\u001b[1;32m    212\u001b[0m join \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformatter\u001b[39m.\u001b[39m_join_parts\n\u001b[0;32m--> 213\u001b[0m item_help \u001b[39m=\u001b[39m join([func(\u001b[39m*\u001b[39margs) \u001b[39mfor\u001b[39;00m func, args \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems])\n\u001b[1;32m    214\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformatter\u001b[39m.\u001b[39m_dedent()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/argparse.py:213\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformatter\u001b[39m.\u001b[39m_indent()\n\u001b[1;32m    212\u001b[0m join \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformatter\u001b[39m.\u001b[39m_join_parts\n\u001b[0;32m--> 213\u001b[0m item_help \u001b[39m=\u001b[39m join([func(\u001b[39m*\u001b[39;49margs) \u001b[39mfor\u001b[39;00m func, args \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems])\n\u001b[1;32m    214\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformatter\u001b[39m.\u001b[39m_dedent()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/argparse.py:338\u001b[0m, in \u001b[0;36mHelpFormatter._format_usage\u001b[0;34m(self, usage, actions, groups, prefix)\u001b[0m\n\u001b[1;32m    336\u001b[0m pos_parts \u001b[39m=\u001b[39m _re\u001b[39m.\u001b[39mfindall(part_regexp, pos_usage)\n\u001b[1;32m    337\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(opt_parts) \u001b[39m==\u001b[39m opt_usage\n\u001b[0;32m--> 338\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(pos_parts) \u001b[39m==\u001b[39m pos_usage\n\u001b[1;32m    340\u001b[0m \u001b[39m# helper for wrapping lines\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_lines\u001b[39m(parts, indent, prefix\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Reranking module based on PolyEncoder')\n",
    "parser.add_argument('train',\n",
    "                    action='store_true',\n",
    "                    default=True,\n",
    "                    help='for training')\n",
    "\n",
    "parser.add_argument('data_dir',\n",
    "                    type=str,\n",
    "                    default='/opt/ml/input/data/train_dataset')\n",
    "\n",
    "parser.add_argument(\"pretrained_model\", type=str, default=\"klue/bert-base\")\n",
    "parser.add_argument(\"model_type\", type=str, default=\"poly\")\n",
    "parser.add_argument(\"embed_size\", type=int, default=768)\n",
    "parser.add_argument(\"batch_size\", type=int, default=1)\n",
    "parser.add_argument(\"cand_size\", type=int, default=31)\n",
    "parser.add_argument(\"max_epoch\", type=int, default=1)\n",
    "\n",
    "parser.add_argument(\"pooling_method\", type=str, default=\"first\")\n",
    "\n",
    "parser.add_argument(\"cuda\", \n",
    "                    action='store_true',\n",
    "                    default=True)\n",
    "\n",
    "parser.add_argument(\"gpuid\", nargs='+', type=str, default = 'cuda:0')\n",
    "\n",
    "today = datetime.datetime.now()\n",
    "\n",
    "parser.add_argument(\"model_name\", type=str, default=f\"{today.strftime('%m%d')}_qa\")\n",
    "parser.add_argument(\"model_pt\", type=str, default=f'{MODEL_DIR}/model_last.ckpt')\n",
    "\n",
    "parser = LightningPCModel.add_model_specific_args(parser)\n",
    "parser = Trainer.add_argparse_args(parser)\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "model = LightningPCModel.load_from_checkpoint('model_ckpt/0103_qa-last.ckpt', tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_encoder = model.pc_model.context_encoder\n",
    "cand_encoder = model.pc_model.cand_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from typing import List, NoReturn, Optional, Tuple, Union\n",
    "from lightning_model import LightningPCModel\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertPreTrainedModel, AdamW, TrainingArguments, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset, SequentialSampler)\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f\"[{name}] done in {time.time() - t0:.3f} s\")\n",
    "\n",
    "\n",
    "class DenseRetrieval:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenize_fn,\n",
    "        cfg,\n",
    "        data_path: Optional[str] = \"/opt/ml/input/data/\",\n",
    "        context_path: Optional[str] = \"wikipedia_documents.json\",\n",
    "    ) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            tokenize_fn:\n",
    "                기본 text를 tokenize해주는 함수입니다.\n",
    "                아래와 같은 함수들을 사용할 수 있습니다.\n",
    "                - lambda x: x.split(' ')\n",
    "                - Huggingface Tokenizer\n",
    "                - konlpy.tag의 Mecab\n",
    "            data_path:\n",
    "                데이터가 보관되어 있는 경로입니다.\n",
    "            context_path:\n",
    "                Passage들이 묶여있는 파일명입니다.\n",
    "            data_path/context_path가 존재해야합니다.\n",
    "        Summary:\n",
    "            Passage 파일을 불러오고 TfidfVectorizer를 선언하는 기능을 합니다.\n",
    "        \"\"\"\n",
    "        self.cfg = cfg\n",
    "        self.data_path = data_path\n",
    "        with open(os.path.join(data_path, context_path), \"r\", encoding=\"utf-8\") as f:\n",
    "            wiki = json.load(f)\n",
    "\n",
    "        self.contexts = list(\n",
    "            dict.fromkeys([v[\"text\"] for v in wiki.values()])\n",
    "        )  # set 은 매번 순서가 바뀌므로\n",
    "        print(f\"Lengths of unique contexts : {len(self.contexts)}\")\n",
    "        self.ids = list(range(len(self.contexts)))\n",
    "        self.p_embedding = None  # get_passage_embedding()로 생성합니다\n",
    "        self.indexer = None  # build_faiss()로 생성합니다.\n",
    "        \n",
    "        train_dataset = load_from_disk(os.path.join(data_path, 'train_dataset'))\n",
    "        self.train_dataset = train_dataset['train']\n",
    "        self.args = TrainingArguments(\n",
    "                    output_dir=\"dense_retrieval\",\n",
    "                    evaluation_strategy=\"epoch\",\n",
    "                    learning_rate=cfg.encoder.lr,\n",
    "                    per_device_train_batch_size=cfg.encoder.batch_size,\n",
    "                    per_device_eval_batch_size=cfg.encoder.batch_size,\n",
    "                    num_train_epochs=cfg.encoder.epoch,\n",
    "                    weight_decay=cfg.encoder.weight_decay\n",
    "                )\n",
    "        model_checkpoint = cfg.encoder.model_name\n",
    "        tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "        if cfg.encoder.ckpt == True:\n",
    "            model = LightningPCModel.load_from_checkpoint('model_ckpt/0103_qa-last.ckpt', tokenizer = tokenizer)\n",
    "            self.p_encoder = model.pc_model.cand_encoder\n",
    "            self.q_encoder = model.pc_model.cand_encoder\n",
    "        if torch.cuda.is_available():\n",
    "            self.p_encoder.cuda()\n",
    "            self.q_encoder.cuda()\n",
    "    def get_passage_embedding(self) -> NoReturn:\n",
    "\n",
    "        \"\"\"\n",
    "        Summary:\n",
    "            Passage Embedding을 만들고\n",
    "            TFIDF와 Embedding을 pickle로 저장합니다.\n",
    "            만약 미리 저장된 파일이 있으면 저장된 pickle을 불러옵니다.\n",
    "        \"\"\"\n",
    "\n",
    "        # Pickle을 저장합니다.\n",
    "        pickle_name = self.cfg.encoder.embedding_name\n",
    "        emd_path = os.path.join(self.data_path, pickle_name)\n",
    "\n",
    "        if os.path.isfile(emd_path):\n",
    "            with open(emd_path, \"rb\") as file:\n",
    "                self.p_embedding = pickle.load(file)\n",
    "            print(\"Embedding pickle load.\")\n",
    "\n",
    "            print(\"Build passage embedding\")\n",
    "            eval_batch_size = 8\n",
    "\n",
    "            # Construt dataloader\n",
    "            valid_p_seqs = self.tokenizer(self.contexts, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "            valid_dataset = TensorDataset(valid_p_seqs['input_ids'], valid_p_seqs['attention_mask'], valid_p_seqs['token_type_ids'])\n",
    "            valid_sampler = SequentialSampler(valid_dataset)\n",
    "            valid_dataloader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "            # Inference using the passage encoder to get dense embeddeings\n",
    "            p_embs = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                epoch_iterator = tqdm(valid_dataloader, desc=\"Iteration\", position=0, leave=True)\n",
    "                self.p_encoder.eval()\n",
    "                for _, batch in enumerate(epoch_iterator):\n",
    "                    batch = tuple(t.cuda() for t in batch)\n",
    "\n",
    "                    p_inputs = {'input_ids': batch[0],\n",
    "                                'attention_mask': batch[1],\n",
    "                                'token_type_ids': batch[2]\n",
    "                                }\n",
    "                        \n",
    "                    outputs = self.p_encoder(**p_inputs).to('cpu').numpy()\n",
    "                    p_embs.extend(outputs)\n",
    "            if self.cfg.encoder.faiss_gpu:\n",
    "                self.p_embedding = p_embs\n",
    "            else:\n",
    "                self.p_embedding = np.array(p_embs)\n",
    "                print(self.p_embedding.shape)\n",
    "\n",
    "            with open(emd_path, \"wb\") as file:\n",
    "                pickle.dump(self.p_embedding, file)\n",
    "            print(\"Embedding pickle saved.\")\n",
    "\n",
    "    def build_faiss(self, num_clusters=64) -> NoReturn:\n",
    "        \"\"\"\n",
    "        Summary:\n",
    "            속성으로 저장되어 있는 Passage Embedding을\n",
    "            Faiss indexer에 fitting 시켜놓습니다.\n",
    "            이렇게 저장된 indexer는 `get_relevant_doc`에서 유사도를 계산하는데 사용됩니다.\n",
    "        Note:\n",
    "            Faiss는 Build하는데 시간이 오래 걸리기 때문에,\n",
    "            매번 새롭게 build하는 것은 비효율적입니다.\n",
    "            그렇기 때문에 build된 index 파일을 저정하고 다음에 사용할 때 불러옵니다.\n",
    "            다만 이 index 파일은 용량이 1.4Gb+ 이기 때문에 여러 num_clusters로 시험해보고\n",
    "            제일 적절한 것을 제외하고 모두 삭제하는 것을 권장합니다.\n",
    "        \"\"\"\n",
    "\n",
    "        indexer_name = f\"faiss_clusters{num_clusters}.index\"\n",
    "        indexer_path = os.path.join(self.data_path, indexer_name)\n",
    "        if os.path.isfile(indexer_path):\n",
    "            print(\"Load Saved Faiss Indexer.\")\n",
    "            self.indexer = faiss.read_index(indexer_path)\n",
    "\n",
    "        else:\n",
    "            p_emb = self.p_embedding.astype(np.float32).toarray()\n",
    "            emb_dim = p_emb.shape[-1]\n",
    "\n",
    "            num_clusters = num_clusters\n",
    "            if self.cfg.encoder.faiss_gpu: \n",
    "                res = faiss.StandardGpuResources()\n",
    "            quantizer = faiss.IndexFlatL2(emb_dim)\n",
    "            if self.cfg.encoder.faiss_gpu:\n",
    "                index_ivf = faiss.IndexIVFScalarQuantizer(\n",
    "                quantizer, quantizer.d, num_clusters, faiss.METRIC_L2\n",
    "            )\n",
    "                self.indexer = faiss.index_cpu_to_gpu(res, 0, index_ivf)\n",
    "            else:\n",
    "                self.indexer = faiss.IndexIVFScalarQuantizer(\n",
    "                    quantizer, quantizer.d, num_clusters, faiss.METRIC_L2\n",
    "                )\n",
    "            self.indexer.train(p_emb)\n",
    "            self.indexer.add(p_emb)\n",
    "            faiss.write_index(self.indexer, indexer_path)\n",
    "            print(\"Faiss Indexer Saved.\")\n",
    "\n",
    "    def retrieve_faiss(\n",
    "        self, query_or_dataset: Union[str, Dataset], topk: Optional[int] = 1\n",
    "    ) -> Union[Tuple[List, List], pd.DataFrame]:\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            query_or_dataset (Union[str, Dataset]):\n",
    "                str이나 Dataset으로 이루어진 Query를 받습니다.\n",
    "                str 형태인 하나의 query만 받으면 `get_relevant_doc`을 통해 유사도를 구합니다.\n",
    "                Dataset 형태는 query를 포함한 HF.Dataset을 받습니다.\n",
    "                이 경우 `get_relevant_doc_bulk`를 통해 유사도를 구합니다.\n",
    "            topk (Optional[int], optional): Defaults to 1.\n",
    "                상위 몇 개의 passage를 사용할 것인지 지정합니다.\n",
    "        Returns:\n",
    "            1개의 Query를 받는 경우  -> Tuple(List, List)\n",
    "            다수의 Query를 받는 경우 -> pd.DataFrame: [description]\n",
    "        Note:\n",
    "            다수의 Query를 받는 경우,\n",
    "                Ground Truth가 있는 Query (train/valid) -> 기존 Ground Truth Passage를 같이 반환합니다.\n",
    "                Ground Truth가 없는 Query (test) -> Retrieval한 Passage만 반환합니다.\n",
    "            retrieve와 같은 기능을 하지만 faiss.indexer를 사용합니다.\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.indexer is not None, \"build_faiss()를 먼저 수행해주세요.\"\n",
    "\n",
    "        if isinstance(query_or_dataset, str):\n",
    "            doc_scores, doc_indices = self.get_relevant_doc_faiss(\n",
    "                query_or_dataset, k=topk\n",
    "            )\n",
    "            print(\"[Search query]\\n\", query_or_dataset, \"\\n\")\n",
    "\n",
    "            for i in range(topk):\n",
    "                print(\"Top-%d passage with score %.4f\" % (i + 1, doc_scores[i]))\n",
    "                print(self.contexts[doc_indices[i]])\n",
    "\n",
    "            return (doc_scores, [self.contexts[doc_indices[i]] for i in range(topk)])\n",
    "\n",
    "        elif isinstance(query_or_dataset, Dataset):\n",
    "            # Retrieve한 Passage를 pd.DataFrame으로 반환합니다.\n",
    "            queries = query_or_dataset[\"question\"]\n",
    "            total = []\n",
    "\n",
    "            with timer(\"query faiss search\"):\n",
    "                doc_scores, doc_indices = self.get_relevant_doc_bulk_faiss(\n",
    "                    queries, k=topk\n",
    "                )\n",
    "            for idx, example in enumerate(\n",
    "                tqdm(query_or_dataset, desc=\"Dense retrieval: \")\n",
    "            ):\n",
    "                tmp = {\n",
    "                    # Query와 해당 id를 반환합니다.\n",
    "                    \"question\": example[\"question\"],\n",
    "                    \"id\": example[\"id\"],\n",
    "                    # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "                    \"context\": \" \".join(\n",
    "                        [self.contexts[pid] for pid in doc_indices[idx]]\n",
    "                    ),\n",
    "                }\n",
    "                if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "                    # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "                    tmp[\"original_context\"] = example[\"context\"]\n",
    "                    tmp[\"answers\"] = example[\"answers\"]\n",
    "                total.append(tmp)\n",
    "\n",
    "            return pd.DataFrame(total)\n",
    "\n",
    "    def get_relevant_doc_faiss(\n",
    "        self, query: str, k: Optional[int] = 1\n",
    "    ) -> Tuple[List, List]:\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            query (str):\n",
    "                하나의 Query를 받습니다.\n",
    "            k (Optional[int]): 1\n",
    "                상위 몇 개의 Passage를 반환할지 정합니다.\n",
    "        Note:\n",
    "            vocab 에 없는 이상한 단어로 query 하는 경우 assertion 발생 (예) 뙣뙇?\n",
    "        \"\"\"\n",
    "\n",
    "        q_seqs = self.tokenizer([query], padding=\"max_length\", truncation=True, return_tensors='pt').to('cuda')\n",
    "        with torch.no_grad():\n",
    "            self.q_encoder.eval()\n",
    "            q_embs = self.q_encoder(**q_seqs).to('cpu').numpy()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # q_embs = q_embs.toarray().astype(np.float32)\n",
    "        with timer(\"query faiss search\"):\n",
    "            D, I = self.indexer.search(q_embs, k)\n",
    "\n",
    "        return D.tolist()[0], I.tolist()[0]\n",
    "\n",
    "    def get_relevant_doc_bulk_faiss(\n",
    "        self, queries: List, k: Optional[int] = 1\n",
    "    ) -> Tuple[List, List]:\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            queries (List):\n",
    "                하나의 Query를 받습니다.\n",
    "            k (Optional[int]): 1\n",
    "                상위 몇 개의 Passage를 반환할지 정합니다.\n",
    "        Note:\n",
    "            vocab 에 없는 이상한 단어로 query 하는 경우 assertion 발생 (예) 뙣뙇?\n",
    "        \"\"\"\n",
    "\n",
    "        q_seqs = self.tokenizer(queries, padding=\"max_length\", truncation=True, return_tensors='pt').to('cuda')\n",
    "        with torch.no_grad():\n",
    "            self.q_encoder.eval()\n",
    "            q_embs = self.q_encoder(**q_seqs).to('cpu').numpy()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        q_embs = q_embs.astype(np.float32)\n",
    "        D, I = self.indexer.search(q_embs, k)\n",
    "\n",
    "        return D.tolist(), I.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  4 2020, 07:30:14) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
