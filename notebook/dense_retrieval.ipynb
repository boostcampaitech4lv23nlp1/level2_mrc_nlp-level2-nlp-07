{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6996eb77-f08b-4c2b-a198-c7fc54498c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertPreTrainedModel, AdamW, TrainingArguments, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import (DataLoader, RandomSampler, TensorDataset, SequentialSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "448a2500-b5dc-44bc-af8f-ed17c68d4710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__'])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['train'].features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d205c721-30dc-45c3-bb39-5245832a4f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_from_disk('/opt/ml/input/data/train_dataset')\n",
    "test_dataset = load_from_disk('/opt/ml/input/data/test_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84c3a49b-092b-4c48-ab56-09019a108bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__'],\n",
       "    num_rows: 3952\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20e04591-f5b3-495c-b710-e3f372afb87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3340"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = list(set([example['context'] for example in train_dataset['train']]))\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fa69047-deab-443b-bad1-d405b55c1280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 15.0kB/s]\n",
      "Downloading: 100%|██████████| 625/625 [00:00<00:00, 291kB/s]\n",
      "Downloading: 100%|██████████| 996k/996k [00:01<00:00, 736kB/s]  \n",
      "Downloading: 100%|██████████| 1.96M/1.96M [00:01<00:00, 1.42MB/s]\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "634d30a2-ceb0-4d0b-8126-3a86fae70627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use subset (128 examples) of original training dataset \n",
    "sample_idx = np.random.choice(range(len(train_dataset['train'])), 128)\n",
    "training_dataset = train_dataset['train'][sample_idx]\n",
    "\n",
    "q_seqs = tokenizer(training_dataset['question'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "p_seqs = tokenizer(training_dataset['context'], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "\n",
    "train_dataset = TensorDataset(p_seqs['input_ids'], p_seqs['attention_mask'], p_seqs['token_type_ids'], \n",
    "                        q_seqs['input_ids'], q_seqs['attention_mask'], q_seqs['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "68ac002d-c4c1-4a8b-afc9-3ed4149d2651",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.per_device_train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "580dd3f0-eab8-4d40-9528-4512fe086eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.sampler.RandomSampler at 0x7f9d967c6fa0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eef9ec34-7775-4a88-99b6-fc49280d6092",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(BertPreTrainedModel):\n",
    "  def __init__(self, config):\n",
    "    super(BertEncoder, self).__init__(config)\n",
    " \n",
    "    self.bert = BertModel(config)\n",
    "    self.init_weights()\n",
    "      \n",
    "  def forward(self, input_ids, \n",
    "              attention_mask=None, token_type_ids=None): \n",
    "  \n",
    "      outputs = self.bert(input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          token_type_ids=token_type_ids)\n",
    "      \n",
    "      pooled_output = outputs[1]\n",
    " \n",
    "      return pooled_output\n",
    " \n",
    " \n",
    "def train(args, dataset, p_model, q_model):\n",
    "  \n",
    "  # Dataloader\n",
    "  train_sampler = RandomSampler(dataset)\n",
    "  train_dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=args.per_device_train_batch_size)\n",
    " \n",
    "  # Optimizer\n",
    "  no_decay = ['bias', 'LayerNorm.weight']\n",
    "  optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in p_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in p_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in q_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in q_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "  optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "  t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n",
    " \n",
    "  # Start training!\n",
    "  global_step = 0\n",
    "  \n",
    "  p_model.zero_grad()\n",
    "  q_model.zero_grad()\n",
    "  torch.cuda.empty_cache()\n",
    "  \n",
    "  train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    " \n",
    "  for _ in train_iterator:\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    " \n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "      q_encoder.train()\n",
    "      p_encoder.train()\n",
    "      \n",
    "      if torch.cuda.is_available():\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    " \n",
    "      p_inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'token_type_ids': batch[2]\n",
    "                  }\n",
    "      \n",
    "      q_inputs = {'input_ids': batch[3],\n",
    "                  'attention_mask': batch[4],\n",
    "                  'token_type_ids': batch[5]}\n",
    "      \n",
    "      p_outputs = p_model(**p_inputs)  # (batch_size, emb_dim)\n",
    "      q_outputs = q_model(**q_inputs)  # (batch_size, emb_dim)\n",
    " \n",
    " \n",
    "      # Calculate similarity score & loss\n",
    "      sim_scores = torch.matmul(q_ouatputs, torch.transpose(p_outputs, 0, 1))  # (batch_size, emb_dim) x (emb_dim, batch_size) = (batch_size, batch_size)\n",
    " \n",
    "      # target: position of positive samples = diagonal element \n",
    "      targets = torch.arange(0, args.per_device_train_batch_size).long()\n",
    "      if torch.cuda.is_available():\n",
    "        targets = targets.to('cuda')\n",
    " \n",
    "      sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    " \n",
    "      loss = F.nll_loss(sim_scores, targets)\n",
    " \n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      scheduler.step()\n",
    "      q_model.zero_grad()\n",
    "      p_model.zero_grad()\n",
    "      global_step += 1\n",
    "      \n",
    "      torch.cuda.empty_cache()\n",
    " \n",
    " \n",
    "    \n",
    "  return p_model, q_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "afac7482-b70a-4190-a1e1-111030ad0fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"dense_retireval\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "707124f9-a335-4053-a90f-55534adee80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 714M/714M [00:11<00:00, 60.6MB/s] \n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained model on cuda (if available)\n",
    "p_encoder = BertEncoder.from_pretrained(model_checkpoint)\n",
    "q_encoder = BertEncoder.from_pretrained(model_checkpoint)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  p_encoder.cuda()\n",
    "  q_encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d585d01-64da-4287-afb2-37ca35793ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/opt/ml/input/data/\"\n",
    "context_path = \"wikipedia_documents.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dea9530-4282-4305-b027-bd11d58ace04",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, context_path), \"r\", encoding=\"utf-8\") as f:\n",
    "    wiki = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0e4c4ed7-e29f-4d44-b428-fd02dc5e5fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of unique contexts : 56737\n"
     ]
    }
   ],
   "source": [
    "search_corpus = list(dict.fromkeys([v[\"text\"] for v in wiki.values()]))  # set 은 매번 순서가 바뀌므로\n",
    "print(f\"Lengths of unique contexts : {len(contexts)}\")\n",
    "ids = list(range(len(contexts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c1eb11e4-a884-4644-9cd1-6b6117f34524",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 7093/7093 [09:41<00:00, 12.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(56737, 768)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_batch_size = 8\n",
    "\n",
    "# Construt dataloader\n",
    "valid_p_seqs = tokenizer(search_corpus, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "valid_dataset = TensorDataset(valid_p_seqs['input_ids'], valid_p_seqs['attention_mask'], valid_p_seqs['token_type_ids'])\n",
    "valid_sampler = SequentialSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "# Inference using the passage encoder to get dense embeddeings\n",
    "p_embs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "  epoch_iterator = tqdm(valid_dataloader, desc=\"Iteration\", position=0, leave=True)\n",
    "  p_encoder.eval()\n",
    "\n",
    "  for _, batch in enumerate(epoch_iterator):\n",
    "    batch = tuple(t.cuda() for t in batch)\n",
    "\n",
    "    p_inputs = {'input_ids': batch[0],\n",
    "                'attention_mask': batch[1],\n",
    "                'token_type_ids': batch[2]\n",
    "                }\n",
    "        \n",
    "    outputs = p_encoder(**p_inputs).to('cpu').numpy()\n",
    "    p_embs.extend(outputs)\n",
    "\n",
    "p_embs = np.array(p_embs)\n",
    "p_embs.shape  # (num_passage, emb_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "42618ed2-471e-405e-a8fa-e3509cb3b61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['로마의 공성무기에 대한 기록을 남긴 사람은?',\n",
       " '전단이 연나라와의 전쟁에서 승리했을 당시 제나라의 왕은 누구인가?',\n",
       " '외국어영화상 위원회에서 최종 후보 다섯 편을 추리는 방법은?',\n",
       " '교황의 문장에서 교차한 금빛 열쇠와 은빛 열쇠가 뜻하는 바는?',\n",
       " '왕필과 함께 대표적인 현학자로 불리며 《장자》에 주석을 단 사람은?']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "sample_idx = np.random.choice(range(len(train_dataset['validation'])), 5)\n",
    "query = train_dataset['validation'][sample_idx]['question']\n",
    "ground_truth = train_dataset['validation'][sample_idx]['context']\n",
    "\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a70aead1-23f5-4664-b600-a527b4418f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 768)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_q_seqs = tokenizer(query, padding=\"max_length\", truncation=True, return_tensors='pt').to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "  q_encoder.eval()\n",
    "  q_embs = q_encoder(**valid_q_seqs).to('cpu').numpy()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "q_embs.shape  # (num_query, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "602ff15d-d617-4c30-8970-5d020c9ffc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  p_embs_cuda = torch.Tensor(p_embs).to('cuda')\n",
    "  q_embs_cuda = torch.Tensor(q_embs).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "77f2750e-fe85-485b-a13c-5c33939ae689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[54667, 19803, 55063,  ..., 38335, 23533, 38653],\n",
      "        [54667, 19803, 55063,  ..., 38335, 23533, 38653],\n",
      "        [54667, 19803, 55063,  ..., 38335, 23533, 38653],\n",
      "        [54667, 19803, 55063,  ..., 38335, 23533, 38653],\n",
      "        [54667, 19803, 55063,  ..., 38335, 23533, 38653]], device='cuda:0')\n",
      "--- 0.010508298873901367 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "dot_prod_scores = torch.matmul(q_embs_cuda, torch.transpose(p_embs_cuda, 0, 1))\n",
    "\n",
    "rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()\n",
    "print(rank)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6bede9c3-4b3e-4212-bb96-2294f70992c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Search query]\n",
      " 로마의 공성무기에 대한 기록을 남긴 사람은? \n",
      "\n",
      "[Ground truth passage]\n",
      "후기 로마군\\n로마 군단 목록\\n \\n로마 제국 말기에, 군단의 수는 늘어났고 로마군은 확장됐다. 일반적으로 인용된 서류상 병력들보다 수가 작아졌다는 증거는 존재하지만, 사두정치 이전에 군단이 형태에 변화가 있었는지를 나타내는 증거는 없다. 군단의 최종 형태는 디오클레티아누스와 사두정치 때 만든 정예 부대 레기오네스 팔라티나이에서 기원했다. 이 부대는 옛 군단의 5,000명 규모보다는 기병을 포함해, 대략 1,000명 규모의 보병 부대이다. 초창기 레기오네스 팔라티나이는 란키아리이, 요비아니, 에르쿨리아니, 디비텐세스 등의 부대가 있었다.\\n\\n4세기에 콘스탄티누스 2세 제위 시기에 시작된 과정으로, 수 많은 수의 새롭고 소규모의 군단들이 창설됐다. 정예 부대 팔라티니외에도, 아욱실리아 팔라티나라는 보조병들과 더불어, 코미타텐세스, 프세우도코미타텐세스라는 군단들이 제정 말기 로마에 보병들을 제공했다. 노티티아 디그니타툼은 야전 부대들인 팔라티나이 군단 25개, 코미타텐세스 군단 70개, 프세우도코미타텐세스 군단 47개, 아욱실리아 팔라티나 111개와 국경 수비 부대 47개 군단을 언급한다. 노티티아 디그니타툼에서 발견된 호노리아니, 그라티아넨세스 같은 군단 명칭들은 새로운 군단들을 만드는 과정이 단발성의 사건이라기보다는 4세기 전반에 지속되었다는 것을 시사한다. 이 명칭들은 또한 많은 새로운 군단들이 벡실라티오네스 또는 옛 군단에서 만들어졌음을 말해준다. 그외에, 벡실라티오네스 팔라티니 24개, 벡실라티오네스 코미타텐세스 73개가 있었고, 그 밖에 부대들이 동방 지역의 리미타네이에 305개, 서방 지역의 리미타네이에 181개가 있었다. 제정 초기 군단과 6세기 이후 군단 사이에 명백한 직접적인 연속성에 대한 드문 예는 기원전 43년에 창설된 마케도니카 제5군단으로, 노티티아 디그니타툼에 마케도니카 5군단이라는 이름의 코미타텐세 군단으로 기록되었고 637년에 아랍인들에게 이집트가 정복될 때까지 활동했다. \\n\\n후기 로마 작가 베게티우스의 군사학 논고에 따르면, 각 켄투리아는 발리스타 한 대, 각 코호르스는 오나거 한 대가 있었고, 군단에는 발리스타 59대와 오나거 10대로 된 강력한 공성무기 행렬을 배정했으며, 이 각각의 공성무기들은 리브리토르스 (공성병) 10명이 조종하고 황소나 노새가 끄는 수레에 실어 이동된다. 도시 및 요새화 시설을 공격하는 것 말고도, 공성무기들은 로마의 요새 및 요새화된 주둔지 (카스트라)를 방어하는 데에 도움을 주는 데 사용되었다. 심지어는 적절한 시기, 특히나 제정 말기에 전투 또는 도하 상황을 지원할 때 야전포 역할을 하기도 했다.\\n\\n수 많은 체계 변화에도, 군단 제도는 서로마 제국의 멸망까지 살아남았고, 이라클리오스 황제가 병력에 대한 수요 증가를 처리하기 시작한 개혁이 테마 제도를 일으킨 7세까지 동로마 제국내에서 지속되었다. 이러한 개혁에도, 동로마/비잔티움의 군대는 초기 로마 군단의 영향을 받은 걸 유지했고, 유사한 훈련 수준, 전술 역량, 조직 등을 지속해나갔다. \n",
      "\n",
      "Top-1 passage with score 189.4968\n",
      "From a peak of population in 1910, the county had declined through 1990. In the early part of the 20th century, particularly from 1910 to 1930, and from 1940 to 1970, it was affected by the Great Migration of blacks out of the segregated society for jobs and opportunities in Midwest and later, West Coast cities. From 1910 to 1920, the population declined more than 17%, as may be seen from the Census table at right. Particularly in the early 20th century, Blacks left to escape the oppression and violence associated with Jim Crow, lynchings, and their disenfranchisement after 1890.\n",
      "\n",
      "From 1940 to 1960, the population declined by more than 29%. Rural whites also left in those years, but a much greater number of African Americans migrated to other areas. After 1930 they became a minority in the county. In 2000, they constituted nearly 43% of the population.\n",
      "\n",
      "As of the 2010 United States Census, there were 13,131 people living in the county. 57.7% were White, 41.3% Black or African American, 0.2% Native American, 0.1% Asian, 0.2% of some other race and 0.6% of two or more races. 0.8% were Hispanic or Latino (of any race).\n",
      "\n",
      "As of the census  of 2000, there were 13,599 people, 5,271 households, and 3,879 families living in the county. The population density was 19 people per square mile (7/km). There were 6,446 housing units at an average density of 9 per square mile (3/km). The racial makeup of the county was 56.42% White, 42.65% Black or African American, 0.13% Native American, 0.08% Asian, 0.01% Pacific Islander, 0.21% from other races, and 0.49% from two or more races. 0.83% of the population were Hispanic or Latino of any race.\n",
      "\n",
      "There were 5,271 households out of which 31.20% had children under the age of 18 living with them, 53.10% were married couples living together, 16.30% had a female householder with no husband present, and 26.40% were non-families. 24.50% of all households were made up of individuals and 12.00% had someone living alone who was 65 years of age or older. The average household size was 2.58 and the average family size was 3.06.\n",
      "\n",
      "In the county, the population was spread out with 26.00% under the age of 18, 8.50% from 18 to 24, 25.60% from 25 to 44, 24.60% from 45 to 64, and 15.30% who were 65 years of age or older. The median age was 38 years. For every 100 females, there were 93.30 males. For every 100 females age 18 and over, there were 89.80 males.\n",
      "\n",
      "The median income for a household in the county was $26,033, and the median income for a family was $31,256. Males had a median income of $28,306 versus $16,173 for females. The per capita income for the county was $14,048. About 19.30% of families and 22.60% of the population were below the poverty line, including 29.70% of those under age 18 and 22.20% of those age 65 or over.\n",
      "Top-2 passage with score 166.4014\n",
      "The city of Cáceres is located in the province of Cáceres, in the Extremadura region of western central Spain.\n",
      "The city has a mediterranean climate (Köppen: Csa) which is tempered by its proximity to the Atlantic Ocean. In winter the average temperature does not exceed °C|0|abbr=on maximum, reaching °C|0|abbr=on minimum, with some frost. In summer the average maximum temperature is °C|0|abbr=on and the average minimum is °C|0|abbr=on. Rainfall is abundant in the months of October, November, March, April and May, but very intermittent.\n",
      "Top-3 passage with score 163.5429\n",
      "아브렐은 사우스캐롤라이나주에 있는 해병대 관련 시설에서 신병 훈련을 받은 데 이어 노스캐롤라이나주에 있는 해병대 기지 캠프에 소총병으로 배치되었다. 이후 한동안 기지에서 다른 해병들과 함께 대규모 수륙양용훈련을 하다가 한국 전쟁이 일어난 뒤 1950년 8월 17일 미국 샌디에고에서 제1해병사단 제1해병연대와 함께 일본의 고베시로 가는 USS 노블에 탑승하였다. 이후 노블은 9월 9일 고베에서 9월 15일 인천 상륙 작전을 하기 위해 9월 13일에 한국의 앞바다에 도착하였다.\n",
      "\n",
      "1950년 9월 15일부터 19일까지 인천 상륙 작전과 서울, 원산, 장진호, 함흥 등지에서 1해병사단 1해병연대 2대대 E 중대와 함께 전투애 참여하였다. 이후 1951년 6월 10일 화천에서 폭발로 사망하였고, 명예 훈장이 추서되었다. 한편 아브렐은 인디애나주 파머스버그에 있는 웨스트론 공동묘지에 묻혔다.\n",
      "Top-4 passage with score 161.7727\n",
      "의회는 당시 대통령이던 엔리코 데 니콜라(Enrico De Nicola)에게 1948년 대선에 출마할 것을 권했다. 하지만 데 니콜라는 출마하지 않기로 하였고, 이에 따라 에이나우디가 대통령으로 당선되어 1955년까지 재직하였다. 그는 대통령의 임기를 7년으로 규정하는 이탈리아에서, 최초로 7년 임기를 모두 채운 인물이 되었다. 1955년 임기 종료와 함께, 종신 상원의원이 되었다. 에이나우디는 다양한 문화, 경제, 대학 기관에서 활동하기도 했다. 그는 열렬한 유럽 연방주의 지지자였다.\n",
      "\n",
      "에이나우디는 개인적으로 돌리아니(Dogliani)에 위치한 자신의 농장에서 네비올로 와인을 생산하면서 이탈리아의 농업을 개발하였다. 에이나우디가 대통령으로 재직하던 1950년, 칸디도(Candido)는 대통령의 주변에 거대한 네비올로 와인이 여러 병 있는 그림을 그려 왕실을 풍자하기도 했다(사진). 이 그림은 법원으로부터 모독죄 판결을 받았고, 관리자였던 조바니노 구아레스키(Giovannino Guareschi)는 이에 책임을 물어야만 했다.\n",
      "\n",
      "에이나우디는 1961년 10월 30일, 로마에서 조용히 눈을 감았다. 향년 87세.\n",
      "Top-5 passage with score 159.3135\n",
      "퇴임에서 볼드윈의 세월은 조용하였다. 네빌 체임벌린이 사망하면서 전쟁 이전의 유화 정책에서 볼드윈의 지각된 부분은 제2차 세계 대전이 일어난 동안과 그 후에 그를 인기없는 인물로 만들었다. 신문의 캠페인은 그를 전쟁 생산에 자신의 시골 저택의 철문을 기부하지 않은 것으로 사냥하였다. 전쟁이 일어난 동안 윈스턴 처칠은 에이먼 데 벌레라의 아일랜드의 지속적인 중립을 향한 더욱 힘든 경향을 취하는 영국의 조언에 그를 단 한번 상담하였다.\\n\\n1945년 6월 부인 루시 여사가 사망하였다. 이제 볼드윈 자신은 관절염을 겪어 걸어다는 데 지팡이가 필요하였다. 조지 5세의 동상의 공개식에 1947년 런던에서 자신의 최종 공개적인 출연을 이루었다. 관중들은 전직 총리를 알아주어 그를 응원하였으나 이 당시 볼드윈은 귀머거리였고, 그들에게 \"당신들은 나를 야유합니까?\"라고 의문하였다. 1930년 케임브리지 대학교의 총장으로 만들어진 그는 1947년 12월 14일 80세의 나이에 우스터셔주 스투어포트온세번 근처 애슬리홀에서 수면 중 자신의 사망까지 이 수용력에 지속하였다. 그는 화장되었고, 그의 재는 우스터 대성당에 안치되었다.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k = 5 \n",
    "\n",
    "for i, q in enumerate(query[:1]):\n",
    "  print(\"[Search query]\\n\", q, \"\\n\")\n",
    "  print(\"[Ground truth passage]\")\n",
    "  print(ground_truth[i], \"\\n\")\n",
    "\n",
    "  r = rank[i]\n",
    "  for j in range(k):\n",
    "    print(\"Top-%d passage with score %.4f\" % (j+1, dot_prod_scores[i][r[j]]))\n",
    "    print(search_corpus[r[j]])\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877fef8-13e9-4031-bbdc-f72ddd02257d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
